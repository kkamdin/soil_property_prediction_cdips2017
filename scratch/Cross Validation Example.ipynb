{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/training.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_columns = [column for column in train.columns if column.startswith('m')]\n",
    "wavenumbers = [float(column.lstrip('m')) for column in data_columns]\n",
    "\n",
    "output_columns = [\"Ca\",\"P\",\"pH\",\"SOC\",\"Sand\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression on the entire dataset gets a perfect score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below runs linear regression on the full dataset and then computes the score . The score is in terms of $R^2$, which is the fraction of squared error we remove by using our prediction instead of just predicting the average value for each output, ignoring the input data. A perfect score is therefore $1$, the score of our baseline model is $0$, and really terrible models, like always guessing $-100$, get negative scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[data_columns].as_matrix()\n",
    "y = train[output_columns].as_matrix()\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "model.fit(X,y)\n",
    "\n",
    "print(model.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But cross-validation shows that the generalization is not great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below runs `numSplits` iterations of cross-validation with an 80-20 train-test split. It uses the `train_test_split` function from `sklearn.model_selection`.\n",
    "\n",
    "The cell beneath it plots the results using `distplot`.\n",
    "\n",
    "The difference between the error on the training set and the error on the test set is called the \"generalization error\", since it arises from us learning the specificities of the training set -- properties that don't generalize to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "numSplits = 50\n",
    "\n",
    "X = train[data_columns].as_matrix()\n",
    "y = train[output_columns].as_matrix()\n",
    "\n",
    "for split in range(numSplits):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.2,)\n",
    "    model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    scores.append(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(scores,kde=True,hist=False,rug=True);\n",
    "plt.suptitle(\"Validation Scores w Linear Regression\",\n",
    "            fontweight='bold',fontsize='xx-large'\n",
    "            );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO regularization doesn't save us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, folks fight over-fitting issues by \"regularizing\" their model -- adding in assumptions about what kinds of solutions should be found. Unfortunately, most standard regularization techniques won't help us. As an example, I chose LASSO regularization. The assumption of LASSO is that most parameters should be 0, with only a subset actually contributing. The size of that subset you expect to find is a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "numSplits = 50\n",
    "\n",
    "X = train[data_columns].as_matrix()\n",
    "y = train[output_columns].as_matrix()\n",
    "\n",
    "for split in range(numSplits):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.2,)\n",
    "    model = sklearn.linear_model.Lasso()\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    scores.append(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(scores,kde=True,hist=False,rug=True);\n",
    "plt.suptitle(\"Validation Scores w LASSO Regression\",\n",
    "            fontweight='bold',fontsize='xx-large'\n",
    "            );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Looking at our residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's helpful to take a look at our mistakes, especially when our models aren't working. Our mistakes are called residuals (they are the \"residue\" left over when we take out the component we've modeled). \n",
    "\n",
    "There's lots of information in the residuals -- they often tell us when our model is wildly incorrect in a way that the score, which is just a summary of the resiudals, can't.\n",
    "Check out\n",
    "[Anscombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)\n",
    "for some examples.\n",
    "\n",
    "The key thing for this little section is to realize that linear regression assumes that, once we've fit our model, the residuals look like a Gaussian distribution.\n",
    "\n",
    "The cells below calculate the residuals for two models.\n",
    "1. A model that just predicts the mean of each variable, ignoring the data. This is the \"strawman\" model that we use to calculate our $R^2$ score.\n",
    "1. A linear regression model trained on 80% of the data.\n",
    "\n",
    "They then store the residuals in a dataframe and plot them with `distplot`. I highly recommend someone pick through these (e.g. at the level of individual output variables). Also, this code is fairly portable, so I recommend you use it to visualize the residuals of any models you try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train[data_columns].as_matrix()\n",
    "y = train[output_columns].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.2,)\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "outputs = model.predict(X_test)\n",
    "LR_residuals = y_test-outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "means = np.mean(y_test,axis=0)\n",
    "\n",
    "mean_residuals = y_test-means\n",
    "\n",
    "results = pd.DataFrame(data=np.vstack([mean_residuals,LR_residuals]),\n",
    "                       columns=output_columns\n",
    "                      )\n",
    "\n",
    "results['model'] = ['mean']*len(mean_residuals)+['LinearRegression']*len(LR_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xMax = max(np.max(mean_residuals),np.max(LR_residuals))\n",
    "xMin = min(np.min(mean_residuals),np.min(LR_residuals))\n",
    "\n",
    "for model in ['mean','LinearRegression']:\n",
    "    plt.figure(figsize=(16,4));\n",
    "    plt.suptitle('residuals for '+model,\n",
    "                 fontweight='bold',fontsize='xx-large'\n",
    "                );\n",
    "    for column in output_columns:\n",
    "        sns.distplot(results[results.model==model][column],\n",
    "                     hist=True,rug=False,kde=False,\n",
    "                     hist_kws={'histtype':'stepfilled',\n",
    "                              'normed':True,},\n",
    "                     label=column\n",
    "                    )\n",
    "        plt.xlabel('true output - predicted output',\n",
    "                  fontweight='bold',fontsize='large'\n",
    "                  )\n",
    "        plt.ylabel('density')\n",
    "        \n",
    "    plt.gca().set_xlim([xMin,xMax])\n",
    "    plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
