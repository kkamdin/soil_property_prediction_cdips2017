{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/training.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_columns = [column for column in train.columns if column.startswith('m')]\n",
    "wavenumbers = [float(column.lstrip('m')) for column in data_columns]\n",
    "\n",
    "output_columns = [\"Ca\",\"P\",\"pH\",\"SOC\",\"Sand\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Dimensionality Reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the cell above indicates, we have more numbers observed for each datapoint than we have datapoints. This will lead many models, like linear regression, to \"overfit\" the data -- they will perfectly predict the outputs for data they've seen, but perform poorly on unseen data.\n",
    "\n",
    "We can resolve this by summarizing the numbers we did observe for each datapoint with a smaller set of numbers. Since the size of the collection of numbers we're using is called the \"dimension\", this process is called \"dimensionality reduction\". For now, this notebook only covers a dimensionality reduction technique called \"Principal Components Analysis\", or PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - Optimal Dimensionality Reduction for Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal components analysis performs the best possible (i.e. information-loss-minimizing) transformation for multivariate Gaussian data (a linear mixture of independent Gaussians).\n",
    "\n",
    "It does this by computing the covariance matrix of the data and then \"diagonalizing\" it -- transforming the data so that, if we calculate the covariance matrix after transforming, the values outside the diagonal are 0. This is achieved by rotating the data vectors.\n",
    "\n",
    "This is also called decorrelation for the following reason: these values correspond to the covariances of datapoints with each other, and the correlation is just the covariance normalized between 0 and 1. Zero covariance therefore means no correlation.\n",
    "\n",
    "Diagonalizing a matrix requires finding its eigenvectors. These eigenvectors are called the \"principal components\" of the data. They have the property that, if we sort them by eigenvalue, projecting our data onto the first $k$ eigenvectors gives us the $k$-dimensional projection of our data that has the highest variance. For Gaussian data, this is equivalent to preserving the information present in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_PCA = sklearn.decomposition.PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train[data_columns].as_matrix()\n",
    "y = train[output_columns].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_centered_X = X-np.mean(X,axis=1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_PCA.fit(zero_centered_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well our PCA is performing, we calculate the remaining variance once the data has been projected onto the given component (performed by taking the dot product of the data point and the component).\n",
    "\n",
    "This is called the \"variance explained\" by the component (though it might make more sense to call it the \"variance retained\").\n",
    "\n",
    "The cell below plots the variance explained (as a fraction of the total variance) for all the components and then the first 120, 60, 30, and 12 components. I've done this because the fall-off is steep -- only the first few components have a variance explained that's far away from zero. This is great news, since it means we can perform really intense dimensionality reduction without fear of losing lots of useful information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_components = len(full_PCA.explained_variance_ratio_)\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "for idx,num_components in enumerate([total_components,120,60,30,12]):\n",
    "    plt.subplot(2,3,idx+1)\n",
    "    plt.plot(np.arange(1,num_components+1,1),\n",
    "             full_PCA.explained_variance_ratio_[:num_components]);\n",
    "    plt.title(str(num_components)+\" components\",\n",
    "            #fontsize='x-large',\n",
    "            );\n",
    "    \n",
    "plt.suptitle('Fraction Variance Explained By Each Component',\n",
    "             fontweight='bold',\n",
    "             fontsize='xx-large',\n",
    "             y=1.05)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below plots much of the same information, but now plots the variance retained when we project the data onto the number of components on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_components = len(full_PCA.explained_variance_ratio_)\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "for idx,num_components in enumerate([total_components,120,60,30,12]):\n",
    "    plt.subplot(2,3,idx+1)\n",
    "    plt.plot(np.arange(1,num_components+1,1),\n",
    "             np.cumsum(full_PCA.explained_variance_ratio_[:num_components])\n",
    "                      );\n",
    "    plt.title(str(num_components)+\" components\",\n",
    "            #fontsize='x-large',\n",
    "            );\n",
    "    plt.ylim(0,1)\n",
    "    \n",
    "plt.suptitle('Cumulative Fraction of Variance Explained',\n",
    "             fontweight='bold',fontsize='xx-large',y=1.05)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the PCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below grabs the PCs selected with `indices_to_plot` and plots them in a grid. It's possible that these spectra are meaningful, and it'd be great if we could determine that!\n",
    "\n",
    "The \"transformed\" data values are just the dot products between the data value and each principal component. These are called the \"loadings\" onto each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_plot = list(range(9))\n",
    "number_to_plot = len(indices_to_plot)\n",
    "components_to_plot = full_PCA.components_[indices_to_plot,:]\n",
    "\n",
    "nrows = int(np.ceil(np.sqrt(number_to_plot)))\n",
    "ncols = nrows\n",
    "\n",
    "fig,axes = plt.subplots(nrows=nrows,ncols=ncols,\n",
    "             sharex=True,sharey=True,\n",
    "             figsize=(20,12),\n",
    "            )\n",
    "for ax,component in zip(np.ravel(axes),components_to_plot):\n",
    "    ax.plot(wavenumbers,component)\n",
    "    \n",
    "plt.suptitle('First ' +str(number_to_plot) + ' Principal Components',\n",
    "            fontweight='bold',fontsize=48,y=1.1,\n",
    "            );\n",
    "plt.tight_layout()\n",
    "\n",
    "explained_variance_percent = np.sum(full_PCA.explained_variance_ratio_[indices_to_plot])*100\n",
    "print(\"these components collectively explain {:0.2f}% of the variance\".format(explained_variance_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do our reconstructions compare to the originals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"reconstruct\" a datapoint that we reduced to a lower dimension by multiplying each of the principal components by the loading of that datapoint onto that component. Because of the rapid rise in the cumulative variance retained, we can get excellent reconstructions while only keeping a small number of PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = 3\n",
    "\n",
    "compressive_PCA = sklearn.decomposition.PCA(n_components=to_keep).fit(zero_centered_X)\n",
    "explained_variance_percent = np.sum(compressive_PCA.explained_variance_ratio_)*100\n",
    "print(\"these components collectively explain {:0.2f}% of the variance\".format(explained_variance_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed_data = compressive_PCA.transform(zero_centered_X)\n",
    "reconstructions = compressive_PCA.inverse_transform(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Repeatedly to Compare Random Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProTip: enter \"Command Mode\" by pressing `Esc` and then hit `h` (for `h`elp) to see a list of keyboard shortcuts. One of them should let you run a cell while keeping it selected, which is useful for cells like the one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numDataPoints = transformed_data.shape[0]\n",
    "\n",
    "random_idx = np.random.choice(numDataPoints)\n",
    "reconstruction = reconstructions[random_idx,:]\n",
    "original = zero_centered_X[random_idx,:]\n",
    "\n",
    "plt.plot(wavenumbers,reconstruction,label='recon');\n",
    "plt.plot(wavenumbers,original,label='orig');\n",
    "plt.suptitle('Example Reconstruction with '+str(to_keep)+' components',\n",
    "            fontweight='bold',fontsize='xx-large',\n",
    "            );\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Linear Regression on top of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below perform linear regression on the PCA-transformed data values.\n",
    "\n",
    "The results are surprisingly good for this dataset! Despite throwing out 99% of the data, we still get a decent score, because PCA helped us determine the right 1% to keep. A good extension for this section would be to add in cross-validation so that the results can be fairly compared with the un-reduced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = 12\n",
    "\n",
    "compressive_PCA = sklearn.decomposition.PCA(n_components=to_keep).fit(zero_centered_X)\n",
    "explained_variance_percent = np.sum(compressive_PCA.explained_variance_ratio_)*100\n",
    "print(\"these components collectively explain {:0.2f}% of the variance\".format(explained_variance_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed_data = compressive_PCA.transform(zero_centered_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR = sklearn.linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.fit(transformed_data,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.score(transformed_data,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
