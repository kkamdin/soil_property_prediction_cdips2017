{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Dimensionality Reduction and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements cross-validation for dimensionality reduction and $\\ell^1$ and $\\ell^2$ regularization in a linear model.\n",
    "\n",
    "The conclusion is that, to get the best performing a linear model, we should retain between 20 and 100 PCs and not use either of the common forms of weight regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/training.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_columns = [column for column in train.columns if column.startswith('m')]\n",
    "wavenumbers = [float(column.lstrip('m')) for column in data_columns]\n",
    "\n",
    "output_columns = [\"Ca\",\"P\",\"pH\",\"SOC\",\"Sand\"]\n",
    "\n",
    "X = train[data_columns].as_matrix()\n",
    "y = train[output_columns].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Dimensionality Reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the cell above indicates, we have more numbers observed for each datapoint than we have datapoints. This will lead many models, like linear regression, to \"overfit\" the data -- they will perfectly predict the outputs for data they've seen, but perform poorly on unseen data. Errors that arise from being good at a specific training set but bad at unseen data in general are called *generalization errors*.\n",
    "\n",
    "We'd like to reduce our generalization errors. Usually, this will involve performing worse on the training set, but reducing the difference between performance on the test and training set, which can lead to better scores on the test set.\n",
    "\n",
    "One way to achieve this is by summarizing the numbers we did observe for each datapoint with a smaller set of numbers. Since the size of the collection of numbers we're using is called the \"dimension\", this process is called \"dimensionality reduction\". For now, this notebook only covers a dimensionality reduction technique called \"Principal Components Analysis\", or PCA. See the \"Dimensionality Reduction\" notebook for more on PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validating PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA, like almost all dimensionality reduction methods, has a free parameter that is not determined by the data -- the number of dimensions that we choose to keep. A parameter that is not determined by the data is called a *hyperparameter*, because it is a parameter that controls the values we get for the rest of the parameters, and so it is above, *hyper-*, the other parameters.\n",
    "\n",
    "One of the easiest and most widely used ways to set the appropriate values for hyperparameters is a technique called *cross-validation*. Check out the \"Cross-Validation\" notebook for more details.\n",
    "\n",
    "The cells below implement cross-validation on PCA. We first select a schedule of dimensions to keep, then we produce PCA transforms for each number of dimensions, and then we run a number of cross-validation splits for each model. The scores are then aggregated across splits and plotted. In the first few cells, we only do one cross-validation split. \n",
    "\n",
    "If the number of splits or the length of the schedule is high, these can take a few minutes. I wouldn't recommend using more than 30 splits, since the estimates of the generalization error are quite good after only a dozen or so splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep_schedule = [1,2,3,5,\n",
    "                    10,20,30,50,\n",
    "                    100,200,300,500,\n",
    "                    #1000,\n",
    "                    1157\n",
    "                   ]\n",
    "compressive_PCAs = []\n",
    "\n",
    "for to_keep in to_keep_schedule:\n",
    "    compressive_PCAs.append(sklearn.decomposition.PCA(n_components=to_keep).fit(X))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,)\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for compressive_PCA in compressive_PCAs:\n",
    "    \n",
    "    transformed_X_train = compressive_PCA.transform(X_train)\n",
    "    transformed_X_test =  compressive_PCA.transform(X_test)\n",
    "    \n",
    "    model.fit(transformed_X_train,y_train)\n",
    "    train_score = model.score(transformed_X_train,y_train)\n",
    "    test_score = model.score(transformed_X_test,y_test)\n",
    "    \n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.semilogx(to_keep_schedule,train_scores,\n",
    "         linewidth=4,alpha=0.75,\n",
    "         label='Train')\n",
    "plt.semilogx(to_keep_schedule,test_scores,\n",
    "         linewidth=4,alpha=0.75,\n",
    "         label='Test');\n",
    "plt.xlabel(\"Retained Dimensions\");\n",
    "plt.ylabel(\"$R^2$\")\n",
    "plt.legend(); plt.title(\"Train vs. Test Scores for PCA-DR\");\n",
    "\n",
    "best_score_index = np.argmax(test_scores)\n",
    "best_score = train_scores[best_score_index]\n",
    "best_score_num_dimensions = to_keep_schedule[best_score_index]\n",
    "print(\"the best number of dimensions to keep is: \"+ str(best_score_num_dimensions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the above cell more than once, you'll get different values for the best number of dimensions to keep!\n",
    "\n",
    "This is because the score on the test set is a random variable, just like the data values and statistics computed from those values, like the mean and standard error and $p$-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "num_splits = 25\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,)\n",
    "\n",
    "train_scores = np.zeros((num_splits,len(to_keep_schedule)))\n",
    "test_scores = np.zeros((num_splits,len(to_keep_schedule)))\n",
    "\n",
    "for pca_idx, compressive_PCA in enumerate(compressive_PCAs):\n",
    "    \n",
    "    for split_idx in range(num_splits):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,)\n",
    "        \n",
    "        transformed_X_train = compressive_PCA.transform(X_train)\n",
    "        transformed_X_test =  compressive_PCA.transform(X_test)\n",
    "    \n",
    "        model.fit(transformed_X_train,y_train)\n",
    "        \n",
    "        train_score = model.score(transformed_X_train,y_train)\n",
    "        test_score = model.score(transformed_X_test,y_test)\n",
    "    \n",
    "        train_scores[split_idx,pca_idx] = train_score\n",
    "        test_scores[split_idx,pca_idx] = test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_train_scores = np.mean(train_scores,axis=0)\n",
    "mean_test_scores = np.mean(test_scores,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.semilogx(to_keep_schedule,mean_train_scores,\n",
    "         linewidth=4,alpha=0.75,\n",
    "         label='Train')\n",
    "plt.semilogx(to_keep_schedule,mean_test_scores,\n",
    "         linewidth=4,alpha=0.75,\n",
    "         label='Test');\n",
    "\n",
    "plt.xlabel(\"Retained Dimensions\");\n",
    "plt.ylabel(\"$R^2$\")\n",
    "plt.legend(); plt.title(\"Train vs. Test Scores for PCA-DR\");\n",
    "\n",
    "best_score_index = np.argmax(mean_test_scores)\n",
    "best_score = mean_test_scores[best_score_index]\n",
    "best_score_num_dimensions = to_keep_schedule[best_score_index]\n",
    "print(\"the best number of dimensions to keep is: \"+ str(best_score_num_dimensions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But recall that we didn't test all of the values -- just the ones in our schedule.\n",
    "\n",
    "We need to examine the region between 10 and 100 more closely. Let's wrap everything we've done up to this point into  a collection of functions so that we can modify it more easily.\n",
    "\n",
    "While we're at it, let's add error bars to our plots so that we get a sense of the stability of our measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runCV(num_splits,compressive_PCAs,X,y,model):\n",
    "    \n",
    "    train_scores = np.zeros((num_splits,len(compressive_PCAs)))\n",
    "    test_scores = np.zeros((num_splits,len(compressive_PCAs)))\n",
    "\n",
    "    for pca_idx, compressive_PCA in enumerate(compressive_PCAs):\n",
    "\n",
    "        for split_idx in range(num_splits):\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.2,)\n",
    "\n",
    "            transformed_X_train = compressive_PCA.transform(X_train)\n",
    "            transformed_X_test =  compressive_PCA.transform(X_test)\n",
    "\n",
    "            model.fit(transformed_X_train,y_train)\n",
    "\n",
    "            train_score = model.score(transformed_X_train,y_train)\n",
    "            test_score = model.score(transformed_X_test,y_test)\n",
    "\n",
    "            train_scores[split_idx,pca_idx] = train_score\n",
    "            test_scores[split_idx,pca_idx] = test_score\n",
    "            \n",
    "    return train_scores, test_scores\n",
    "\n",
    "def PCAsFromSchedule(to_keep_schedule,X):\n",
    "    \n",
    "    PCAs = []\n",
    "    \n",
    "    for to_keep in to_keep_schedule:\n",
    "        PCAs.append(sklearn.decomposition.PCA(n_components=to_keep).fit(X))\n",
    "    \n",
    "    return PCAs\n",
    "\n",
    "def makePlot(schedule,train_scores,test_scores):\n",
    "    \n",
    "    mean_train_scores = np.mean(train_scores,axis=0)\n",
    "    mean_test_scores = np.mean(test_scores,axis=0)\n",
    "\n",
    "    sd_train_scores = np.std(train_scores,axis=0,ddof=1)\n",
    "    sd_test_scores = np.std(test_scores,axis=0,ddof=1)\n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_xscale(\"log\", nonposx='clip')\n",
    "    \n",
    "    plt.errorbar(schedule,mean_train_scores,\n",
    "                 yerr=sd_train_scores,\n",
    "             linewidth=4,alpha=0.75,\n",
    "             label='Train')\n",
    "    \n",
    "    plt.errorbar(schedule,mean_test_scores,\n",
    "                 yerr=sd_test_scores,\n",
    "             linewidth=4,alpha=0.75,\n",
    "             label='Test')\n",
    "    \n",
    "    plt.ylim([0,1]);\n",
    "\n",
    "    plt.xlabel(\"Retained Dimensions\");\n",
    "    plt.ylabel(\"$R^2$\")\n",
    "    plt.legend(); plt.title(\"Train vs. Test Scores for PCA-DR\");\n",
    "    \n",
    "def getBest(test_scores,to_keep_schedule):\n",
    "    \n",
    "    mean_test_scores = np.mean(test_scores,axis=0)\n",
    "    \n",
    "    best_score_index = np.argmax(mean_test_scores)\n",
    "    best_score = mean_test_scores[best_score_index]\n",
    "    best_score_num_dimensions = to_keep_schedule[best_score_index]\n",
    "    print(\"the best number of dimensions to keep is: \"+ str(best_score_num_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produceCVPlot(to_keep_schedule,num_splits,\n",
    "                  X,y,\n",
    "                  model=sklearn.linear_model.LinearRegression()):\n",
    "    \n",
    "    compressive_PCAs = PCAsFromSchedule(to_keep_schedule,X)\n",
    "    \n",
    "    train_scores, test_scores = runCV(num_splits,compressive_PCAs,X,y,model)\n",
    "    \n",
    "    makePlot(to_keep_schedule,train_scores,test_scores)\n",
    "    getBest(test_scores,to_keep_schedule)\n",
    "    \n",
    "    return train_scores,test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep_schedule = [10,20,30,40,50,\n",
    "                    60,70,80,90,\n",
    "                    100,110,120,130,\n",
    "                    140,150,160,170,\n",
    "                    180,190,200\n",
    "                   ]\n",
    "num_splits = 25\n",
    "\n",
    "train_scores, test_scores = produceCVPlot(to_keep_schedule,num_splits,\n",
    "             X,y,\n",
    "             );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On close inspection, it appears that the raw cross-validation scores don't give a way to choose between PCA dimensionality reductions to any number of dimensions between roughly 20 and roughly 100 dimensions.\n",
    "\n",
    "The general preference, based on the Occam's razor principle, is for a simpler, rather than a more complex, model, if both models are equally supported by the data. Simplicity in our case corresponds to retaining fewer dimensions, so we should probably keep a number of dimensions in between 20 and 50, rather than between 50 and 100.\n",
    "\n",
    "There are methods that attempt to quantify the heuristic above -- the\n",
    "[Aikaike Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) and the\n",
    "[Bayesian Information Criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion). `sklearn` implements both of these, so it's common to use them directly.\n",
    "\n",
    "However, both criteria rely on the model class being used to reduce the dimensionality to match the process that generates the data. Here, we have no reason to really believe that our data is, in fact, multivariate Gaussian, so we should not necessarily trust the numbers provided by AIC and BIC. As such, it's probably best to stick with (quantitatively-guided) heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is not the only technique we can use to reduce generalization error.\n",
    "\n",
    "In general, schemes that fight overfitting are called \"regularizers\". Many regularizers work by adding terms to the function we use to compute the score of a model. For example, the most common regularizers add a term to the cost that depends on the values of the parameters, with no reference to the data.\n",
    "\n",
    "The most common class of regularizers that depend on the parameter values is the $\\ell_p$, pronounced \"ell-pee\", class. The terms we add to the model cost look like:\n",
    "\n",
    "$$\n",
    "    \\ell^p(\\text{parameters}) = \\sqrt[p]{\\sum_\\text{parameters} \\lvert\\text{parameter}\\rvert^p}\n",
    "$$\n",
    "\n",
    "Where the vertical lines around a value $x$, e.g. $\\lvert x\\rvert$, mean \"absolute value of $x$\" and $\\sqrt[p]{x}$ means \"the $p$th root of $x$\", the number that, when multiplied with itself $p$ times, gives us $x$.\n",
    "\n",
    "Recall the definition of the distance from the origin for a point $(x,y,z)$ \n",
    "\n",
    "$$\n",
    "    \\text{distance}(x,y,z) = \\sqrt{x^2+y^2+z^2}\n",
    "$$\n",
    "\n",
    "Rewrite this using the $\\Sigma$ notation for sums:\n",
    "\n",
    "$$\n",
    "    \\text{distance}(x,y,z) = \\sqrt{\\sum_{a = x,y,z} a^2} =\\sqrt[2]{\\sum_{a = x,y,z} \\lvert a\\rvert^2}\n",
    "$$\n",
    "\n",
    "This tells us that what we think of as \"distance\" is the same thing as what we're calling here \"$\\ell^2$\". If we use $\\ell^2$ regularization on our parameters, our resulting model will have smaller parameter values. When this form of regularization is applied to a regression problem, it is called *ridge regression*.\n",
    "\n",
    "The other important case is $\\ell^1$ regularization. Let's write out the definition and then simplify it:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\ell^1(\\text{parameters}) &= \\sqrt[1]{\\sum_\\text{parameters} \\lvert\\text{parameter}\\rvert^1} \\\\\n",
    "                              &= \\sum_\\text{parameters} \\lvert\\text{parameter}\\rvert\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is just the sum of the absolute values of the parameters. This cost function tends to encourage some parameters to be exactly $0$. It's nice for when you'd like to make a model that's interpretable -- if the parameter associated with input $x$ is non-zero, then you know that it's important for making good predictions. When this regularizer is applied to regression problems, it's called *LASSO* regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use a regularization scheme that involves adding a term to our model's cost function, we generally introduce a hyperparameter, usually called $\\alpha$ that tells us the relative importance of the original cost function and the regularizer term.\n",
    "\n",
    "The cells below implement both $\\ell^1$ and $\\ell^2$ regularization and look for the best-performing values of $\\alpha$ using cross-validation. Somewhat surprisingly, that value seems to be $0$ for this dataset (at least after we perform dimensionality reduction), indicating that we shouldn't use either of these regularizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\ell^1$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alphaPlot(alphas,train_scores,test_scores,model_type):\n",
    "    \n",
    "    mean_train_scores = np.mean(train_scores,axis=0)\n",
    "    mean_test_scores = np.mean(test_scores,axis=0)\n",
    "\n",
    "    sd_train_scores = np.std(train_scores,axis=0,ddof=1)\n",
    "    sd_test_scores = np.std(test_scores,axis=0,ddof=1)\n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    plt.errorbar(alphas,mean_train_scores,\n",
    "                 yerr=sd_train_scores,\n",
    "             linewidth=4,alpha=0.75,\n",
    "             label='Train')\n",
    "    \n",
    "    plt.errorbar(alphas,mean_test_scores,\n",
    "                 yerr=sd_test_scores,\n",
    "             linewidth=4,alpha=0.75,\n",
    "             label='Test')\n",
    "    \n",
    "    plt.ylim([0,1]);\n",
    "\n",
    "    plt.xlabel(r\"$\\alpha $ value\");\n",
    "    plt.ylabel(\"$R^2$\")\n",
    "    plt.legend(); plt.title(\"Train vs. Test Scores for \"+model_type);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = 50\n",
    "compressivePCA = sklearn.decomposition.PCA(n_components=to_keep).fit(X)\n",
    "\n",
    "num_splits = 25\n",
    "\n",
    "eps = 0.01\n",
    "alphas = np.arange(0,1.0+eps,eps)\n",
    "num_alphas = len(alphas)\n",
    "\n",
    "train_scores = np.zeros((num_splits,num_alphas))\n",
    "test_scores = np.zeros((num_splits,num_alphas))\n",
    "\n",
    "for alpha_idx,alpha in enumerate(alphas):\n",
    "    \n",
    "    train_score, test_score = runCV(num_splits,[compressivePCA],\n",
    "                                     X,y,\n",
    "                                     model=sklearn.linear_model.Lasso(alpha=alpha)\n",
    "                                     );\n",
    "    train_scores[:,alpha_idx] = train_score.flatten()\n",
    "    test_scores[:,alpha_idx] = test_score.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alphaPlot(alphas,train_scores,test_scores,\"LASSO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\ell^2$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_keep = 50\n",
    "compressivePCA = sklearn.decomposition.PCA(n_components=to_keep).fit(X)\n",
    "\n",
    "num_splits = 25\n",
    "\n",
    "eps = 0.1\n",
    "alphas = np.arange(0,1.0+eps,eps)\n",
    "num_alphas = len(alphas)\n",
    "\n",
    "train_scores = np.zeros((num_splits,num_alphas))\n",
    "test_scores = np.zeros((num_splits,num_alphas))\n",
    "\n",
    "for alpha_idx,alpha in enumerate(alphas):\n",
    "    \n",
    "    train_score, test_score = runCV(num_splits,[compressivePCA],\n",
    "                                     X,y,\n",
    "                                     model=sklearn.linear_model.Ridge(alpha=alpha)\n",
    "                                     );\n",
    "    train_scores[:,alpha_idx] = train_score.flatten()\n",
    "    test_scores[:,alpha_idx] = test_score.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alphaPlot(alphas,train_scores,test_scores,\"Ridge\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
