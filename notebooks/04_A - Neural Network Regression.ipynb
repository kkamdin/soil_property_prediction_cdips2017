{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner](../img/cdips_2017_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "\n",
    "import scripts.load_data as load\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "import sklearn.random_projection\n",
    "import sklearn.neural_network\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y  = load.load_training_spectra()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Neural networks](http://pages.cs.wisc.edu/~dyer/cs540/handouts/deep-learning-nature2015.pdf)\n",
    "are a powerful and flexible tool for solving problems\n",
    "ranging from\n",
    "[data science](http://www.oreilly.com/pub/e/2538)\n",
    "to\n",
    "[artificial intelligence](https://en.wikipedia.org/wiki/AlphaGo).\n",
    "\n",
    "There are many different kinds of neural networks,\n",
    "but we'll be focusing on\n",
    "*feedforward neural networks*,\n",
    "also known as\n",
    "[multilayer perceptrons](http://scikit-learn.org/stable/modules/neural_networks_supervised.html).\n",
    "\n",
    "The online book\n",
    "[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)\n",
    "is a great resource for folks\n",
    "with less background in mathematics and statistics,\n",
    "while the online book\n",
    "[Deep Learning](http://www.deeplearningbook.org/)\n",
    "is more authoritative and thorough,\n",
    "but requires more background.\n",
    "\n",
    "If you'd just like to see some neural networks in action\n",
    "on some visualizable datasets,\n",
    "check out Google's\n",
    "[Neural Network Playground](http://playground.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind neural networks is simple:\n",
    "we linearly transform our input,\n",
    "$\\mathbf{x}$,\n",
    "using a matrix, $\\text{W}$,\n",
    "and then apply a nonlinear function $f$\n",
    "to get values we call the\n",
    "*activations*, $\\mathbf{a}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{a} = f(\\text{W}\\mathbf{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These activations can then be passed through another combination\n",
    "of linear and nonlinear transformation,\n",
    "as can those resulting activations.\n",
    "\n",
    "At some point, we compare some final set of activations $\\hat{y}$\n",
    "to some target value $y$\n",
    "using a cost function,\n",
    "just as we do in other models.\n",
    "\n",
    "We then calculate how much the cost function changes if we\n",
    "change each of the values in the various matrices $\\text{W}$.\n",
    "This change is called the *gradient*\n",
    "and the algorithm used to calculate the gradients\n",
    "is called\n",
    "[backpropagation](http://colah.github.io/posts/2015-08-Backprop/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setup was\n",
    "[inspired by the architecture of the brain](https://mitpress.mit.edu/books/parallel-distributed-processing).\n",
    "In the brain,\n",
    "cells called neurons\n",
    "receive inputs,\n",
    "either from sensors of the outside world\n",
    "or from other neurons,\n",
    "which are combined with varying weights\n",
    "and then nonlinearly transformed\n",
    "before being passed on to other neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the simplicity of the linear-nonlinear transformation,\n",
    "neural networks are capable of\n",
    "[implementing any function on their inputs](http://neuralnetworksanddeeplearning.com/chap4.html).\n",
    "\n",
    "This makes them capable of extreme over-fitting,\n",
    "especially on small datasets like ours.\n",
    "\n",
    "To avoid this, we made hyper-parameter choices\n",
    "that would discourage over-fitting.\n",
    "Here are a few of them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Weight Decay** - Weight decay is identical to what is called, for other models,\n",
    "[$\\ell^2$ regularization](link).\n",
    "Weight decay encourages weight values to be small, which can prevent over-fitting solutions,\n",
    "which tend to have high weights.\n",
    "- **Small Minibatches** - Instead of using the entire training set (also called a *batch*) to calculate the current performance and the gradient, we use a very small subset, also called a *minibatch*. This makes the gradient less reliable, but it also makes it harder for the network to fit the training set perfectly. Intuitively, only patterns that are generally useful will show up in every minibatch and so in every gradient, while patterns that are useful for only a small subset of the data will show up infrequently and cancel each other out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the heuristic of trying to avoid over-fitting,\n",
    "we tried, by hand,\n",
    "a number of different hyperparameter settings.\n",
    "\n",
    "The cells below train a neural network\n",
    "with these hand-selected hyperparameters\n",
    "and display its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "handtuned_model = sklearn.neural_network.MLPRegressor(activation = 'relu',\n",
    "                                                     batch_size = 16,\n",
    "                                                     hidden_layer_sizes = (100),\n",
    "                                                     learning_rate_init = 0.0001,\n",
    "                                                     max_iter = 10000,\n",
    "                                                     tol = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_model = sklearn.linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossvalidate_performance(model, X, y, num_splits = 20):\n",
    "\n",
    "    train_scores = np.zeros(num_splits)\n",
    "    test_scores = np.zeros(num_splits)\n",
    "\n",
    "    for split_idx in range(num_splits):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.2,)\n",
    "\n",
    "        PCA_transform = sklearn.decomposition.PCA(n_components=100).fit(X_train)\n",
    "\n",
    "        transformed_X_train = PCA_transform.transform(X_train)\n",
    "        transformed_X_test = PCA_transform.transform(X_test)\n",
    "\n",
    "        model.fit(transformed_X_train, y_train)\n",
    "\n",
    "        train_scores[split_idx] = model.score(transformed_X_train, y_train)\n",
    "        test_scores[split_idx] = model.score(transformed_X_test, y_test)\n",
    "        \n",
    "    return train_scores, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "byhand_train, byhand_test = crossvalidate_performance(handtuned_model, X, y, num_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_train, linear_test = crossvalidate_performance(linear_model, X, y, num_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_performance(model_name, train_scores, test_scores):\n",
    "    means = [np.mean(train_scores), np.mean(test_scores)]\n",
    "    stds = [np.std(train_scores), np.std(test_scores)]\n",
    "    \n",
    "    plt.errorbar([0,1], means, yerr=stds,\n",
    "            linewidth=4,label=model_name)\n",
    "    plt.xticks([0,1], ['Training', 'Test'])\n",
    "    plt.ylim([0,1]); plt.ylabel(r'$R^2$');\n",
    "    plt.xlim([-0.5,1.5])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "display_performance('hand-tuned MLP', byhand_train, byhand_test) \n",
    "display_performance('best linear model', linear_train, linear_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is decent performance, but it's likely we can do better!\n",
    "\n",
    "Instead of selecting hyperparameters by hand\n",
    "and trying new values guided by heuristics,\n",
    "we can perform a search.\n",
    "\n",
    "scikit-learn implements\n",
    "[several search methods](http://scikit-learn.org/stable/modules/grid_search.html).\n",
    "Unfortunately,\n",
    "there are no methods that\n",
    "[unequivocally perform better](http://www.argmin.net/2016/06/20/hypertuning/)\n",
    "than the simplest ones:\n",
    "selecting feasible values for each hyperparameter\n",
    "and then checking all combinations,\n",
    "also known as\n",
    "[grid search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "or checking random values for each hyperparameter\n",
    "within some feasible range,\n",
    "also known as\n",
    "[random search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV).\n",
    "\n",
    "Most folks use random search,\n",
    "as we do in\n",
    "[this notebook](05_A - Gradient Boosted Regression Trees - Single Target.ipynb).\n",
    "To be complete,\n",
    "we here use grid search instead.\n",
    "\n",
    "Performing a cross-validated grid search\n",
    "across all of the hyperparameters for a neural network\n",
    "takes a very long time.\n",
    "In fact, the time grows multiplicatively with each\n",
    "added hyperparameter:\n",
    "if it takes 4 minutes to check all combinations of two hyperparameters\n",
    "that take on just two different values (e.g `True` or `False`),\n",
    "then it takes 8 minutes to check all combinations if we add on another binary hyperparameter.\n",
    "\n",
    "The grid search that was performed for this model required testing\n",
    "around 2500\n",
    "different networks.\n",
    "Each had to be fit to 3 different cross-validation folds in order to get\n",
    "an estimate of the test-set performance,\n",
    "so the search required the training of\n",
    "roughly 7500\n",
    "neural networks.\n",
    "\n",
    "On a consumer laptop, this took over a day.\n",
    "In industrial settings,\n",
    "the amount of computing power available is generally greater,\n",
    "but so is the model complexity and dataset size,\n",
    "so search times of days or weeks are not uncommon.\n",
    "\n",
    "The results of a scikit-learn grid search are stored\n",
    "in a pandas DataFrame.\n",
    "Our results are saved as a `.csv` file\n",
    "in `data/model_params/GridSearch_07262017.csv`.\n",
    "\n",
    "More detail on these results can be found in the\n",
    "[GridSearchCV for Neural Networks](04_B - GridSearchCV for Neural Networks.ipynb)\n",
    "notebook.\n",
    "\n",
    "The cells below produce a cross-validated performance plot\n",
    "for the best setting of the hyperparameters\n",
    "that came out of that grid search.\n",
    "\n",
    "Because the definition of \"best\" is underdetermined,\n",
    "we present two of the networks uncovered by this\n",
    "search process.\n",
    "\n",
    "One is the model with the best performance\n",
    "on the test set.\n",
    "However, our\n",
    "[close analysis of the grid search results](04_B - GridSearchCV for Neural Networks.ipynb)\n",
    "determined that some of the best perfomers on the test set had\n",
    "substantial gaps between their test and train error,\n",
    "also known as *generalization error*.\n",
    "A high generalization error with an internally split train/test\n",
    "set bodes ill for the generalization error on newly-collected data,\n",
    "since novel test data is likely to be even more different from the training set\n",
    "than the artificially-created test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-Search Optimized Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_test_model = sklearn.neural_network.MLPRegressor(activation = 'relu',\n",
    "                                                     alpha = 0.0001,\n",
    "                                                     batch_size = 16,\n",
    "                                                     beta_1 = 0.9,\n",
    "                                                     beta_2= 0.99,\n",
    "                                                     early_stopping = False,\n",
    "                                                     hidden_layer_sizes = 100,\n",
    "                                                     learning_rate_init = 0.0001,\n",
    "                                                     max_iter = 10000,\n",
    "                                                     tol = 1e-16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_test_train, best_test_test = crossvalidate_performance(best_test_model, X, y, num_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_generalizer_model = sklearn.neural_network.MLPRegressor(activation = 'logistic',\n",
    "                                                     alpha = 0.0001,\n",
    "                                                     batch_size = 16,\n",
    "                                                     beta_1 = 0.95,\n",
    "                                                     beta_2= 0.99,\n",
    "                                                     early_stopping = False,\n",
    "                                                     hidden_layer_sizes = 100,\n",
    "                                                     learning_rate_init = 0.0001,\n",
    "                                                     max_iter = 10000,\n",
    "                                                     tol = 1e-16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_generalizer_train, best_generalizer_test = crossvalidate_performance(best_generalizer_model,\n",
    "                                                                          X, y, num_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "display_performance('hand-tuned MLP', byhand_train, byhand_test)\n",
    "display_performance('best-test MLP', best_test_train, best_test_test)\n",
    "display_performance('best-generalizer MLP', best_generalizer_train, best_generalizer_test)\n",
    "display_performance('best linear model', linear_train, linear_test) \n",
    "plt.title(r'Comparison of Train and Test Performance for MLP models');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we don't get a huge\n",
    "boost in performance on the test set from\n",
    "our grid search.\n",
    "\n",
    "A random search might be able to find a better setting of the parameters,\n",
    "but the\n",
    "[results of our grid search](04_B - GridSearchCV for Neural Networks.ipynb)\n",
    "and the\n",
    "Kaggle leaderboard\n",
    "seem to indicate that substantially better performance isn't possible with this training set.\n",
    "\n",
    "With this is mind, it seems that minimizing the gap between\n",
    "training set performance and test set performance\n",
    "seems like the best move,\n",
    "but on that front the linear model is quite competitive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
