{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner](../img/cdips_2017_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Cross-Validation for Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "\n",
    "import scripts.load_data as load\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "import sklearn.random_projection\n",
    "import sklearn.neural_network\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y  = load.load_training_spectra()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the grid search are in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes about the results:\n",
    "\n",
    "- In attempt to match the [Kaggle competition](https://www.kaggle.com/c/afsis-soil-properties#evaluation),\n",
    "we used a different evaluation method than the $R^2$,\n",
    "which is the default for scikit-learn.\n",
    "The evaluation method is\n",
    "mean columnwise root mean squared error (MCRMSE)\n",
    "-- the average, across the five targets, of our root mean squared error.\n",
    "Though the two are closely related,\n",
    "there's not an exact transformation between the two.\n",
    "- GridSearchCV uses a \"score\",\n",
    "rather than an \"error\" --\n",
    "a score goes up when you do better,\n",
    "whereas an error goes down.\n",
    "To match this,\n",
    "we ran the GridSearch with negative MCRMSE as the\n",
    "\"score\".\n",
    "The cells below load the data and then multiply the score\n",
    "values by `-1` so that they become MCRMSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_results = pd.read_csv('../data/model_params/GridSearch_07262017.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_keep = [column for column in grid_search_results.columns\n",
    "                                            if not column.endswith('_time')]\n",
    "\n",
    "grid_search_results = grid_search_results[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_flip = [column for column in grid_search_results.columns\n",
    "                                          if column.endswith('_score')\n",
    "                                              and not (column.startswith('std')\n",
    "                                                      or column.startswith('rank'))]\n",
    "\n",
    "grid_search_results[columns_to_flip] = grid_search_results[columns_to_flip]*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_rename = [column for column in grid_search_results.columns\n",
    "                                            if column.endswith('_score')]\n",
    "\n",
    "renamed_columns = [column.rstrip('score')+'error' for column in columns_to_rename]\n",
    "grid_search_results[renamed_columns] = grid_search_results[columns_to_rename]\n",
    "grid_search_results = grid_search_results.drop(columns_to_rename,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the basic descriptive statistics of the test and train error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_describe = ['mean_test_error','mean_train_error']\n",
    "\n",
    "grid_search_results[columns_to_describe].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution appears to be very skew,\n",
    "with the maximum being 11 orders of magnitude above the minimum\n",
    "and 10 orders of magnitude larger than the 75th percentile.\n",
    "\n",
    "These bad performers will make it difficult to visualize\n",
    "the remaining data, so we drop them\n",
    "by `query`ing for models that have reasonable performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_results = grid_search_results.query('mean_test_error<1')\n",
    "filtered_results = filtered_results.query('mean_train_error<1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(filtered_results.mean_test_error, kde=False,\n",
    "             hist_kws={'normed':True,\n",
    "                       'edgecolor':'k',\n",
    "                       'linewidth':4,\n",
    "                      'histtype':'stepfilled'},\n",
    "            label='test');\n",
    "\n",
    "sns.distplot(filtered_results.mean_train_error, kde=False,\n",
    "             hist_kws={'normed':True,\n",
    "                       'edgecolor':'k',\n",
    "                       'linewidth':4,\n",
    "                      'histtype':'stepfilled'},\n",
    "            label='train');\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel('MCRMS Error',fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, it appears that there's a hard limit to our performance on the test set,\n",
    "while performance on the training set can reach up to nearly perfect.\n",
    "\n",
    "Though it's difficult to compare precisely,\n",
    "since the test set used in the Kaggle competition \n",
    "is now unavailable,\n",
    "this limit to our performance appears close to\n",
    "the best performance possible --\n",
    "within a few hundredths, at least.\n",
    "\n",
    "In addition to the test error,\n",
    "the difference between the test and train error\n",
    "is also an important criterion for model selection.\n",
    "This is called the *generalization error*.\n",
    "\n",
    "Below, we calculate it and plot its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for split_index in ['0','1','2']:\n",
    "    test_error = filtered_results['split'+split_index+'_test_error']\n",
    "    train_error = filtered_results['split'+split_index+'_train_error']\n",
    "    filtered_results['generalization_error'+split_index] = test_error-train_error\n",
    "    \n",
    "filtered_results['mean_generalization_error'] =  1/3*(filtered_results['generalization_error0'] +\n",
    "                                                filtered_results['generalization_error1'] +\n",
    "                                                filtered_results['generalization_error2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(filtered_results.mean_generalization_error, kde=False,\n",
    "             hist_kws={'normed':True,\n",
    "                       'edgecolor':'k',\n",
    "                       'linewidth':4,\n",
    "                      'histtype':'stepfilled'},\n",
    "            label='generalization');\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel('MCRMS Error',fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization performance is very spread out,\n",
    "with most models generalizing very well,\n",
    "but some models generalizing very poorly.\n",
    "\n",
    "In order to pick the best model,\n",
    "we need to look at the relationship between generalization error\n",
    "and test error.\n",
    "\n",
    "The cell below uses seaborn's `pairplot`\n",
    "to plot the pairwise relationships between training error,\n",
    "test error, and generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_plot = [\"mean_test_error\",\"mean_train_error\",\"mean_generalization_error\"]\n",
    "\n",
    "sns.pairplot(filtered_results,vars=columns_to_plot,\n",
    "                plot_kws={'alpha':0.1,},);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and test error are approximately linearly related, as might be expected.\n",
    "\n",
    "The most important plot here is in the top-right corner,\n",
    "and indicates the relationship between generalization error and test error.\n",
    "\n",
    "The cell below plots just this relationship using `jointplot`.\n",
    "To get a clearer sense of the distribution,\n",
    "we plot it with a\n",
    "[kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation),\n",
    "which is a sort of \"smoothed histogram\", loosely speaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x='mean_generalization_error',y='mean_test_error',\n",
    "             data=filtered_results,\n",
    "             stat_func=None,\n",
    "             kind='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best models are in the bottom-left of this chart --\n",
    "they have low test error and low generalization error.\n",
    "\n",
    "It appears that the majority of models tested fall in a single cluster,\n",
    "with more variability along the test error axis\n",
    "than along the generalization error axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a gross sense of the performance in hand,\n",
    "we now proceed to looking at the parameters of the best performers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Performers on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at the top performers on the test set.\n",
    "\n",
    "We can sort by the `rank_test_error` column\n",
    "to pull out the best-performing hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_results = filtered_results.sort_values(by='rank_test_error')\n",
    "filtered_results.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things pop out:\n",
    "for example,\n",
    "early stopping is unpopular\n",
    "and rectified linear units (`relu`s)\n",
    "seem to outperform logistic units.\n",
    "\n",
    "We can get a closer look by\n",
    "plotting the distribution of test performances for each hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_results = filtered_results.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_to_describe = [column for column in best_results.columns\n",
    "                                          if column.startswith('param_')\n",
    "                                          and not (column.endswith('_max_iter')\n",
    "                                                  or column.endswith('_learning_rate_init')\n",
    "                                                  or column.endswith('_alpha')\n",
    "                                                  or column.endswith('_early_stopping'))\n",
    "                                              ]                        \n",
    "\n",
    "def make_error_plot(dataframe, error_column, error_name, columns_to_describe):\n",
    "    plt.figure(figsize=(12,24))\n",
    "    cols = 2\n",
    "    rows = np.ceil(len(columns_to_describe)/2)\n",
    "    for idx,column in enumerate(columns_to_describe):\n",
    "        plt.subplot(rows,cols,idx+1)\n",
    "        sns.stripplot(x=dataframe[column],y=dataframe[error_column],\n",
    "                      jitter=True,size=12,alpha=0.75,color='gray'\n",
    "                     )\n",
    "        sns.violinplot(x=dataframe[column],y=dataframe[error_column],\n",
    "                      )\n",
    "        plt.ylabel(error_name +' MCRMS Error',fontsize='xx-large')\n",
    "        plt.xlabel(column[6:])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(error_name + ' Error Distribution For Different Hyperparameters',y=1.01,\n",
    "                fontweight='bold',fontsize='xx-large');\n",
    "    \n",
    "make_error_plot(best_results, 'mean_test_error', 'Test', columns_to_describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For best test performance,\n",
    "we want rectified linear units\n",
    "with a batch size smaller than 128,\n",
    "100 nodes in the hidden layer,\n",
    "and no early stopping.\n",
    "\n",
    "Interestingly, these are essentially\n",
    "the same parameters that were discovered\n",
    "by the hand-tuning procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks with Lowest Generalization Error from Best Performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_results = best_results.sort_values(by='mean_generalization_error')\n",
    "best_results[columns_to_describe+['mean_generalization_error','mean_test_error','mean_train_error']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very different patterns arise for the generalization error\n",
    "than for the test error!\n",
    "For example, logistic units seem to be better,\n",
    "and batch size doesn't seem to be as important.\n",
    "\n",
    "Plotting the distribution of the generalization error confirms these trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_describe = [column for column in best_results.columns\n",
    "                                          if column.startswith('param_')\n",
    "                                          and not (column.endswith('_max_iter')\n",
    "                                                  or column.endswith('_learning_rate_init')\n",
    "                                                  or column.endswith('_alpha')\n",
    "                                                  or column.endswith('_early_stopping'))\n",
    "                                              ]    \n",
    "\n",
    "make_error_plot(best_results, 'mean_generalization_error', 'Generalization', columns_to_describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So which hyperparameter setting do we choose?\n",
    "\n",
    "Over-fitting is a serious issue for machine learning methods\n",
    "applied in the real world.\n",
    "Importantly,\n",
    "the method of separating a data set collected at one time\n",
    "into a training and a test set tends to underestimate\n",
    "what the generalization error will be\n",
    "when the model is deployed on totally new data.\n",
    "\n",
    "Heuristically, a large generalization error on the\n",
    "internally split training and test set\n",
    "is a sign that the generalization error\n",
    "on a totally novel set will be even larger.\n",
    "This would motivate us to select the hyperparameter settings\n",
    "that minimize generalization error while still\n",
    "keeping the test error low."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
