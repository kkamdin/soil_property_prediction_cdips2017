{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![banner](../img/cdips_2017_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regression - All targets\n",
    "With single target GBR under our belts (see [single target notebook](05_A - Gradient Boosted Regression Trees - Single Target.ipynb), \n",
    "we can now evaluate performance for all 5 targets by performing GBR\n",
    "on each target separately.  We will end up with a model for each target.\n",
    "\n",
    "If you wish to follow the same method of selecting feature importances\n",
    "as in the [single GBR notebook](05_A - Gradient Boosted Regression Trees - Single Target.ipynb), this can be achieved by setting up an\n",
    "skl [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline). \n",
    "A sequence of transformations is easily applied to multiple targets.\n",
    "\n",
    "For simplicity, this notebook takes the first 100 principal components\n",
    "to reduce the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "import scripts.load_data as load\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "import sklearn.decomposition\n",
    "import sklearn.ensemble\n",
    "import sklearn.model_selection\n",
    "import sklearn.feature_selection\n",
    "import sklearn.metrics\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y=load.load_training_spectra()\n",
    "pca = skl.decomposition.PCA(n_components=100)\n",
    "X_transformed = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two functions for different scoring \n",
    "metrics are in the cell below.\n",
    "More information on scoring metrics \n",
    "can be found on [scikit-learn's site](http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score).\n",
    "\n",
    "`scoreGBR` returns $R^2$ as calculated \n",
    "from all targets.  $R^2$ is defined \n",
    "as \n",
    "\n",
    "\\begin{equation}\n",
    "R^2(y,\\hat{y}) = \n",
    "1-\\frac{\\sum (y_i - \\hat{y}_i)^2}\n",
    "       {\\sum (y_i - \\bar{y})^2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{y}$ is the predicted \n",
    "value, $y$ is the true value, \n",
    "and $\\bar{y} = \\frac{1}{n}\\sum{y_i}$.  All \n",
    "sums are over $i$ between $1$ and $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MCRMSE` returns the [kaggle scoring metric](https://www.kaggle.com/c/afsis-soil-properties#evaluation): \n",
    "mean columnwise root mean squared error, \n",
    "the average of the RMSE found for each target. \n",
    "\n",
    "\\begin{equation}\n",
    "MCRMSE = \\frac{1}{5} \\sum_{j=1}^5 \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_{ij} - \\hat{y}_{ij})^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainGBR(GBR_models, X_train, y_train):\n",
    "    for output_idx, GBR_model in enumerate(GBR_models):\n",
    "        GBR_model.fit(X_train, y_train.iloc[:, output_idx])\n",
    "\n",
    "def scoreGBR(GBR_models,X_test,y_test):\n",
    "    \n",
    "    score = np.zeros(len(GBR_models))\n",
    "    y_pred = np.zeros(y_test.shape)\n",
    "    \n",
    "    for output_idx,GBR_model in enumerate(GBR_models):\n",
    "        y = y_test.iloc[:,output_idx]\n",
    "        y_hat = GBR_model.predict(X_test)\n",
    "        y_pred[:,output_idx] = y_hat\n",
    "    \n",
    "    score = sklearn.metrics.r2_score(y_test, y_pred, multioutput='variance_weighted')\n",
    "    \n",
    "    return score\n",
    "\n",
    "#Kaggle scoring metric: mean columnwise root mean square error\n",
    "def MCRMSE(GBR_models, X_test, y_test):\n",
    "    score = np.zeros(len(GBR_models))\n",
    "    y_pred = np.zeros(y_test.shape)\n",
    "    \n",
    "    for output_idx,GBR_model in enumerate(GBR_models):\n",
    "        y = y_test.iloc[:,output_idx]\n",
    "        y_hat = GBR_model.predict(X_test)\n",
    "        y_pred[:,output_idx] = y_hat\n",
    "        score[output_idx]=np.sqrt(skl.metrics.mean_squared_error(y,y_hat))\n",
    "        #print(score[output_idx])\n",
    "   \n",
    "    meanscore = np.mean(score)\n",
    "    #print(meanscore)\n",
    "    return meanscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of hyperparameters below achieve reasonable performance on all 5 targets.\n",
    "For the ambitious, you may be able to get better models by tuning these\n",
    "for each individual target, so that each target gets its own optimal set of\n",
    "hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_outputs = y.shape[1]\n",
    "params = {'n_estimators':5000,\n",
    "          'max_depth':3,\n",
    "          'min_samples_split':15,\n",
    "          'min_samples_leaf':3,\n",
    "          'max_features':0.8,\n",
    "          'learning_rate':0.01}\n",
    "\n",
    "GBR_models = [skl.ensemble.GradientBoostingRegressor(**params) for _ in range(num_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = skl.model_selection.train_test_split(X_transformed,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start=time()\n",
    "trainGBR(GBR_models, X_train, y_train)\n",
    "print(\"GradientBoostingRegressor took %.2f seconds\"\n",
    "      % (time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoreGBR(GBR_models,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoreGBR(GBR_models,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MCRMSE(GBR_models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MCRMSE(GBR_models, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
