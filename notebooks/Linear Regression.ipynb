{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Dimensionality Reduction and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements cross-validation for dimensionality reduction and $\\ell^1$ and $\\ell^2$ regularization in a linear model.\n",
    "\n",
    "The conclusion is that, to get the best performing linear model, we should first reduce the dimensionality of the data with PCA, and retain between 20 and 100 PCs as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from scripts.load_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, load in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_training_spectra()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training data consists of a near-infrared absorption spectrum for each soil sample\n",
    "\n",
    "Each spectrum consists of measurements at 3578 wave numbers. (For more information, see the data visualization notebook (link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also know the quantities of each of the 5 target variables present in each sample\n",
    "\n",
    "In this notebook, we will predict the quantities of these target variables from the spectra using linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a linear model, we will represent the value the target variable y as a linear combination of the features - in this case, our 3578 wavenumbers. In other words, we predict the output $\\hat{y}$ with a linear function of $x$\n",
    "\n",
    "$$\n",
    "        \\hat{y}=w^Tx \\\\\n",
    "$$\n",
    "\n",
    "In which $x$ is a vector of features and $w^T$ is a vector of weights to be given to each feature. In our case, since we are predicting multiple targets, the weights are contained in a matrix $\\text{W}^T$ and our output is a vector of predicted targets $\\hat{Y}$:\n",
    "\n",
    "\n",
    "$$\n",
    "        \\hat{Y}=\\text{W}^T X \\\\\n",
    "$$\n",
    "\n",
    "In which $X$ is a vector of features, $\\text{W}^T$ is a matrix of weights to be given to each feature, and $\\hat{Y}$ is the output vector. \n",
    "\n",
    "Linear regression is a good place to start with this data set because it is conceptually easy to understand and easy to fit, it works pretty well in many cases, and will give us a good baseline to compare our other models to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a linear model\n",
    "\n",
    "Using scikitlearn, we fit this model by changing the values of the weights $\\text{W}^T$ in order to minimize the root-mean-squared-error between our observed target values and the values predicted by the model: \n",
    "\n",
    "$$\n",
    "      \\text{best model} = \\arg\\!\\text{min}_{w}\\  \\sqrt{\\sum_\\text{dataset}\\left(y-w^T x\\right)^2} \\\\\n",
    "$$\n",
    "\n",
    "We will then test how well this model generalizes to data the model hasn't seen by predicting the targets $\\hat{Y}$ of each sample from its inputs $X$. The performance of the model is determined by root-mean-squared-error, or by $R^2$, which represents the proportion of the variance in the observed values of the targets that can be predicted by the model.\n",
    "\n",
    "For more detailed info on fitting linear models, go to https://github.com/charlesfrye/AppliedStatisticsForNeuroscience/tree/master/10%20-%20Model%20Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,)\n",
    "model.fit(X_train,y_train)\n",
    "train_score = model.score(X_train,y_train)\n",
    "test_score = model.score(X_test,y_test)\n",
    "\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The linear model fit to every feature of the data perfectly predicts in the data it's trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict=model.predict(X_train);\n",
    "y_test_predict=model.predict(X_test);\n",
    "\n",
    "y_train_plot=y_train.as_matrix();\n",
    "y_test_plot=y_test.as_matrix();\n",
    "\n",
    "plt.scatter(y_train_plot[:,0],y_train_predict[:,0])\n",
    "plt.xlabel(\"observed\")\n",
    "plt.ylabel(\"predicted\")\n",
    "plt.title(\"Predicted Ca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However, the model doesn't generalize well to the test data:\n",
    "\n",
    "If we plot the observed vs. predicted values for each sample in the test set, we can see that a lot of the variance isn't predicted by the linear model. This is quantified for all targets as $R^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 3,figsize=(16,11))\n",
    "ax=ax.ravel();\n",
    "targets=y.columns;\n",
    "\n",
    "for targ in range(len(targets)):\n",
    "    ax[targ].scatter(y_test_plot[:,targ],y_test_predict[:,targ])\n",
    "    ax[targ].plot([y_test_plot[:,targ].min(), y_test_plot[:,targ].max()],\n",
    "             [y_test_plot[:,targ].min(), y_test_plot[:,targ].max()], 'k--', lw=4)\n",
    "    #ax[targ].xlabel(\"observed\")\n",
    "    #ax[targ].ylabel(\"predicted\")\n",
    "    ax[targ].set_title(targets[targ])\n",
    "\n",
    "    \n",
    "f.text(0.2, 0.04, 'observed', ha='center',fontsize=30)\n",
    "f.text(0.04, 0.3, 'predicted', va='center', rotation='vertical',fontsize=30)\n",
    "f.delaxes(ax[5])\n",
    "f.suptitle(r\"$R^2 $= \" + str(test_score),fontsize=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validate model to better estimate the fit\n",
    "\n",
    "Each time we run and test the model, the score changes. This is because we are randomly splitting our data into training and test sets, and by chance some models will be fit on training sets that more accurately represent the data. To get a better estimate of the performance of the model, we will do K-fold cross-validation and use the best-performing model as a baseline. For more info, see the cross-validation notebook. *add link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "num_splits = 50\n",
    "\n",
    "\n",
    "train_scores = np.zeros(num_splits)\n",
    "test_scores = np.zeros(num_splits)\n",
    "\n",
    "for split_idx in range(num_splits):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,)\n",
    "        \n",
    "     \n",
    "    \n",
    "        model.fit(X_train,y_train)\n",
    "        \n",
    "        train_score = model.score(X_train,y_train)\n",
    "        test_score = model.score(X_test,y_test)\n",
    "    \n",
    "        train_scores[split_idx] = train_score\n",
    "        test_scores[split_idx] = test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the cross-validation scores (plotted below), each model performs poorly on unseen data despite perfectly fitting the data it was trained on. Errors that arise from being good at a specific training set but bad at unseen data in general are called *generalization errors*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot test scores from cross validation\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.hist(test_scores)\n",
    "plt.xlabel(r\"$R^2$\")\n",
    "plt.title(\"Test Data Scores\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the cell above indicates, we have more numbers observed for each datapoint than we have datapoints. This will lead many models, like linear regression, to \"overfit\" the data, leading to generalization errors.\n",
    "\n",
    "We'd like to reduce our generalization errors. Usually, this will involve performing worse on the training set, but reducing the difference between performance on the test and training set, which can lead to better scores on the test set.\n",
    "\n",
    "One way to achieve this is by summarizing the numbers we did observe for each datapoint with a smaller set of numbers - a process called \"dimensionality reduction\". We'll use a common dimensionality reduction technique called \"Principal Components Analysis\", or PCA. See the \"Dimensionality Reduction\" notebook for more on PCA. (add link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validating PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA, like almost all dimensionality reduction methods, has a free parameter that is not determined by the data -- the number of dimensions that we choose to keep. A parameter that is not determined by the data is called a *hyperparameter*, because it is a parameter that controls the values we get for the rest of the parameters, and so it is above, *hyper-*, the other parameters. We will use cross-validation to determine appropriate the appropriate number of dimensions to keep.\n",
    "\n",
    "The cells below implement cross-validation on PCA. We first select a schedule of dimensions to keep, then we produce PCA transforms for each number of dimensions, and then we run a number of cross-validation splits for each model. The scores are then aggregated across splits and plotted. In the first few cells, we only do one cross-validation split. \n",
    "\n",
    "If the number of splits or the length of the schedule is high, these can take a few minutes. I wouldn't recommend using more than 30 splits, since the estimates of the generalization error are quite good after only a dozen or so splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_keep_schedule = [1,2,3,5,\n",
    "                    10,20,30,50,\n",
    "                    100,200,300,500,\n",
    "                    #1000,\n",
    "                    1157\n",
    "                   ]\n",
    "compressive_PCAs = []\n",
    "\n",
    "for to_keep in to_keep_schedule:\n",
    "    compressive_PCAs.append(sklearn.decomposition.PCA(n_components=to_keep).fit(X))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,)\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for compressive_PCA in compressive_PCAs:\n",
    "    \n",
    "    transformed_X_train = compressive_PCA.transform(X_train)\n",
    "    transformed_X_test =  compressive_PCA.transform(X_test)\n",
    "    \n",
    "    model.fit(transformed_X_train,y_train)\n",
    "    train_score = model.score(transformed_X_train,y_train)\n",
    "    test_score = model.score(transformed_X_test,y_test)\n",
    "    \n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.semilogx(to_keep_schedule,train_scores,\n",
    "         linewidth=4,alpha=0.75,\n",
    "         label='Train')\n",
    "plt.semilogx(to_keep_schedule,test_scores,\n",
    "         linewidth=4,alpha=0.75,\n",
    "         label='Test');\n",
    "plt.xlabel(\"Retained Dimensions\");\n",
    "plt.ylabel(\"$R^2$\")\n",
    "plt.legend(); plt.title(\"Train vs. Test Scores for PCA-DR\");\n",
    "\n",
    "best_score_index = np.argmax(test_scores)\n",
    "best_score = train_scores[best_score_index]\n",
    "best_score_num_dimensions = to_keep_schedule[best_score_index]\n",
    "print(\"the best number of dimensions to keep is: \"+ str(best_score_num_dimensions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the above cell more than once, you'll get different values for the best number of dimensions to keep!\n",
    "\n",
    "This is because the score on the test set is a random variable, just like the data values and statistics computed from those values, like the mean and standard error and $p$-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "num_splits = 25\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,)\n",
    "\n",
    "train_scores = np.zeros((num_splits,len(to_keep_schedule)))\n",
    "test_scores = np.zeros((num_splits,len(to_keep_schedule)))\n",
    "\n",
    "for pca_idx, compressive_PCA in enumerate(compressive_PCAs):\n",
    "    \n",
    "    for split_idx in range(num_splits):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,)\n",
    "        \n",
    "        transformed_X_train = compressive_PCA.transform(X_train)\n",
    "        transformed_X_test =  compressive_PCA.transform(X_test)\n",
    "    \n",
    "        model.fit(transformed_X_train,y_train)\n",
    "        \n",
    "        train_score = model.score(transformed_X_train,y_train)\n",
    "        test_score = model.score(transformed_X_test,y_test)\n",
    "    \n",
    "        train_scores[split_idx,pca_idx] = train_score\n",
    "        test_scores[split_idx,pca_idx] = test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_train_scores = np.mean(train_scores,axis=0)\n",
    "mean_test_scores = np.mean(test_scores,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.semilogx(to_keep_schedule,mean_train_scores,\n",
    "         linewidth=4,alpha=0.75,\n",
    "         label='Train')\n",
    "plt.semilogx(to_keep_schedule,mean_test_scores,\n",
    "         linewidth=4,alpha=0.75,\n",
    "         label='Test');\n",
    "\n",
    "plt.xlabel(\"Retained Dimensions\");\n",
    "plt.ylabel(\"$R^2$\")\n",
    "plt.legend(); plt.title(\"Train vs. Test Scores for PCA-DR\");\n",
    "\n",
    "best_score_index = np.argmax(mean_test_scores)\n",
    "best_score = mean_test_scores[best_score_index]\n",
    "best_score_num_dimensions = to_keep_schedule[best_score_index]\n",
    "print(\"the best number of dimensions to keep is: \"+ str(best_score_num_dimensions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But recall that we didn't test all of the values -- just the ones in our schedule.\n",
    "\n",
    "We need to examine the region between 10 and 100 more closely. Let's wrap everything we've done up to this point into  a collection of functions so that we can modify it more easily.\n",
    "\n",
    "While we're at it, let's add error bars to our plots so that we get a sense of the stability of our measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runCV(num_splits,compressive_PCAs,X,y,model):\n",
    "    \n",
    "    train_scores = np.zeros((num_splits,len(compressive_PCAs)))\n",
    "    test_scores = np.zeros((num_splits,len(compressive_PCAs)))\n",
    "\n",
    "    for pca_idx, compressive_PCA in enumerate(compressive_PCAs):\n",
    "\n",
    "        for split_idx in range(num_splits):\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.2,)\n",
    "\n",
    "            transformed_X_train = compressive_PCA.transform(X_train)\n",
    "            transformed_X_test =  compressive_PCA.transform(X_test)\n",
    "\n",
    "            model.fit(transformed_X_train,y_train)\n",
    "\n",
    "            train_score = model.score(transformed_X_train,y_train)\n",
    "            test_score = model.score(transformed_X_test,y_test)\n",
    "\n",
    "            train_scores[split_idx,pca_idx] = train_score\n",
    "            test_scores[split_idx,pca_idx] = test_score\n",
    "            \n",
    "    return train_scores, test_scores\n",
    "\n",
    "def PCAsFromSchedule(to_keep_schedule,X):\n",
    "    \n",
    "    PCAs = []\n",
    "    \n",
    "    for to_keep in to_keep_schedule:\n",
    "        PCAs.append(sklearn.decomposition.PCA(n_components=to_keep).fit(X))\n",
    "    \n",
    "    return PCAs\n",
    "\n",
    "def makePlot(schedule,train_scores,test_scores):\n",
    "    \n",
    "    mean_train_scores = np.mean(train_scores,axis=0)\n",
    "    mean_test_scores = np.mean(test_scores,axis=0)\n",
    "\n",
    "    sd_train_scores = np.std(train_scores,axis=0,ddof=1)\n",
    "    sd_test_scores = np.std(test_scores,axis=0,ddof=1)\n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_xscale(\"log\", nonposx='clip')\n",
    "    \n",
    "    plt.errorbar(schedule,mean_train_scores,\n",
    "                 yerr=sd_train_scores,\n",
    "             linewidth=4,alpha=0.75,\n",
    "             label='Train')\n",
    "    \n",
    "    plt.errorbar(schedule,mean_test_scores,\n",
    "                 yerr=sd_test_scores,\n",
    "             linewidth=4,alpha=0.75,\n",
    "             label='Test')\n",
    "    \n",
    "    plt.ylim([0,1]);\n",
    "\n",
    "    plt.xlabel(\"Retained Dimensions\");\n",
    "    plt.ylabel(\"$R^2$\")\n",
    "    plt.legend(); plt.title(\"Train vs. Test Scores for PCA-DR\");\n",
    "    \n",
    "def getBest(test_scores,to_keep_schedule):\n",
    "    \n",
    "    mean_test_scores = np.mean(test_scores,axis=0)\n",
    "    \n",
    "    best_score_index = np.argmax(mean_test_scores)\n",
    "    best_score = mean_test_scores[best_score_index]\n",
    "    best_score_num_dimensions = to_keep_schedule[best_score_index]\n",
    "    print(\"the best number of dimensions to keep is: \"+ str(best_score_num_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def produceCVPlot(to_keep_schedule,num_splits,\n",
    "                  X,y,\n",
    "                  model=sklearn.linear_model.LinearRegression()):\n",
    "    \n",
    "    compressive_PCAs = PCAsFromSchedule(to_keep_schedule,X)\n",
    "    \n",
    "    train_scores, test_scores = runCV(num_splits,compressive_PCAs,X,y,model)\n",
    "    \n",
    "    makePlot(to_keep_schedule,train_scores,test_scores)\n",
    "    getBest(test_scores,to_keep_schedule)\n",
    "    \n",
    "    return train_scores,test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep_schedule = [10,20,30,40,50,\n",
    "                    60,70,80,90,\n",
    "                    100,110,120,130,\n",
    "                    140,150,160,170,\n",
    "                    180,190,200\n",
    "                   ]\n",
    "num_splits = 25\n",
    "\n",
    "train_scores, test_scores = produceCVPlot(to_keep_schedule,num_splits,\n",
    "             X,y,\n",
    "             );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On close inspection, it appears that the raw cross-validation scores don't give a way to choose between PCA dimensionality reductions to any number of dimensions between roughly 20 and roughly 100 dimensions.\n",
    "\n",
    "The general preference, based on the Occam's razor principle, is for a simpler, rather than a more complex, model, if both models are equally supported by the data. Simplicity in our case corresponds to retaining fewer dimensions, so we should probably keep a number of dimensions in between 20 and 50, rather than between 50 and 100.\n",
    "\n",
    "There are methods that attempt to quantify the heuristic above -- the\n",
    "[Aikaike Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) and the\n",
    "[Bayesian Information Criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion). `sklearn` implements both of these, so it's common to use them directly.\n",
    "\n",
    "However, both criteria rely on the model class being used to reduce the dimensionality to match the process that generates the data. Here, we have no reason to really believe that our data is, in fact, multivariate Gaussian, so we should not necessarily trust the numbers provided by AIC and BIC. As such, it's probably best to stick with (quantitatively-guided) heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is not the only technique we can use to reduce generalization error.\n",
    "\n",
    "In general, schemes that fight overfitting are called \"regularizers\". Many regularizers work by adding terms to the function we use to compute the score of a model. For example, the most common regularizers add a term to the cost that depends on the values of the parameters, with no reference to the data.\n",
    "\n",
    "The most common class of regularizers that depend on the parameter values is the $\\ell_p$, pronounced \"ell-pee\", class. The terms we add to the model cost look like:\n",
    "\n",
    "$$\n",
    "    \\ell^p(\\text{parameters}) = \\sqrt[p]{\\sum_\\text{parameters} \\lvert\\text{parameter}\\rvert^p}\n",
    "$$\n",
    "\n",
    "Where the vertical lines around a value $x$, e.g. $\\lvert x\\rvert$, mean \"absolute value of $x$\" and $\\sqrt[p]{x}$ means \"the $p$th root of $x$\", the number that, when multiplied with itself $p$ times, gives us $x$.\n",
    "\n",
    "Recall the definition of the distance from the origin for a point $(x,y,z)$ \n",
    "\n",
    "$$\n",
    "    \\text{distance}(x,y,z) = \\sqrt{x^2+y^2+z^2}\n",
    "$$\n",
    "\n",
    "Rewrite this using the $\\Sigma$ notation for sums:\n",
    "\n",
    "$$\n",
    "    \\text{distance}(x,y,z) = \\sqrt{\\sum_{a = x,y,z} a^2} =\\sqrt[2]{\\sum_{a = x,y,z} \\lvert a\\rvert^2}\n",
    "$$\n",
    "\n",
    "This tells us that what we think of as \"distance\" is the same thing as what we're calling here \"$\\ell^2$\". If we use $\\ell^2$ regularization on our parameters, our resulting model will have smaller parameter values. When this form of regularization is applied to a regression problem, it is called *ridge regression*.\n",
    "\n",
    "The other important case is $\\ell^1$ regularization. Let's write out the definition and then simplify it:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\ell^1(\\text{parameters}) &= \\sqrt[1]{\\sum_\\text{parameters} \\lvert\\text{parameter}\\rvert^1} \\\\\n",
    "                              &= \\sum_\\text{parameters} \\lvert\\text{parameter}\\rvert\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is just the sum of the absolute values of the parameters. This cost function tends to encourage some parameters to be exactly $0$. It's nice for when you'd like to make a model that's interpretable -- if the parameter associated with input $x$ is non-zero, then you know that it's important for making good predictions. When this regularizer is applied to regression problems, it's called *LASSO* regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use a regularization scheme that involves adding a term to our model's cost function, we generally introduce a hyperparameter, usually called $\\alpha$ that tells us the relative importance of the original cost function and the regularizer term.\n",
    "\n",
    "The cells below implement both $\\ell^1$ and $\\ell^2$ regularization and look for the best-performing values of $\\alpha$ using cross-validation. Somewhat surprisingly, that value seems to be $0$ for this dataset (at least after we perform dimensionality reduction), indicating that we shouldn't use either of these regularizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\ell^1$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alphaPlot(alphas,train_scores,test_scores,model_type):\n",
    "    \n",
    "    mean_train_scores = np.mean(train_scores,axis=0)\n",
    "    mean_test_scores = np.mean(test_scores,axis=0)\n",
    "\n",
    "    sd_train_scores = np.std(train_scores,axis=0,ddof=1)\n",
    "    sd_test_scores = np.std(test_scores,axis=0,ddof=1)\n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    plt.errorbar(alphas,mean_train_scores,\n",
    "                 yerr=sd_train_scores,\n",
    "             linewidth=4,alpha=0.75,\n",
    "             label='Train')\n",
    "    \n",
    "    plt.errorbar(alphas,mean_test_scores,\n",
    "                 yerr=sd_test_scores,\n",
    "             linewidth=4,alpha=0.75,\n",
    "             label='Test')\n",
    "    \n",
    "    plt.ylim([0,1]);\n",
    "\n",
    "    plt.xlabel(r\"$\\alpha $ value\");\n",
    "    plt.ylabel(\"$R^2$\")\n",
    "    plt.legend(); plt.title(\"Train vs. Test Scores for \"+model_type);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = 50\n",
    "compressivePCA = sklearn.decomposition.PCA(n_components=to_keep).fit(X)\n",
    "\n",
    "num_splits = 25\n",
    "\n",
    "eps = 0.01\n",
    "alphas = np.arange(0,1.0+eps,eps)\n",
    "num_alphas = len(alphas)\n",
    "\n",
    "train_scores = np.zeros((num_splits,num_alphas))\n",
    "test_scores = np.zeros((num_splits,num_alphas))\n",
    "\n",
    "for alpha_idx,alpha in enumerate(alphas):\n",
    "    \n",
    "    train_score, test_score = runCV(num_splits,[compressivePCA],\n",
    "                                     X,y,\n",
    "                                     model=sklearn.linear_model.Lasso(alpha=alpha)\n",
    "                                     );\n",
    "    train_scores[:,alpha_idx] = train_score.flatten()\n",
    "    test_scores[:,alpha_idx] = test_score.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alphaPlot(alphas,train_scores,test_scores,\"LASSO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\ell^2$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_keep = 50\n",
    "compressivePCA = sklearn.decomposition.PCA(n_components=to_keep).fit(X)\n",
    "\n",
    "num_splits = 25\n",
    "\n",
    "eps = 0.1\n",
    "alphas = np.arange(0,1.0+eps,eps)\n",
    "num_alphas = len(alphas)\n",
    "\n",
    "train_scores = np.zeros((num_splits,num_alphas))\n",
    "test_scores = np.zeros((num_splits,num_alphas))\n",
    "\n",
    "for alpha_idx,alpha in enumerate(alphas):\n",
    "    \n",
    "    train_score, test_score = runCV(num_splits,[compressivePCA],\n",
    "                                     X,y,\n",
    "                                     model=sklearn.linear_model.Ridge(alpha=alpha)\n",
    "                                     );\n",
    "    train_scores[:,alpha_idx] = train_score.flatten()\n",
    "    test_scores[:,alpha_idx] = test_score.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "alphaPlot(alphas,train_scores,test_scores,\"Ridge\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
